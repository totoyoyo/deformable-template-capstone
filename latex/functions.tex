
\documentclass[12pt]{article}


\usepackage{fancyhdr}
\usepackage[hmargin = 1in,vmargin=1in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{breqn}

\newcommand{\A}{\alpha}
\newcommand{\B}{\beta}
\newcommand{\longones}[1]{
  \begin{bmatrix}
    1\\
    \vdots\\
    1_{#1}
  \end{bmatrix}
}

\DeclareMathSymbol{*}{\mathbin}{symbols}{"01}


\begin{document}

We know that $D = K\beta$, and expanding $K$ yields:

\begin{dmath*}
  D = \exp\left(\frac{-1}{2\cdot sd_g^2} * \left({P^{2}\cdot 
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix} \cdot 
  \longones{K_g}^\top} 
  + {(C_b^{2}\cdot 
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix}
  \cdot 
  \longones{|\Lambda|}
  ^\top )^\top} 
  -2\cdot P\cdot C_b^\top \right)  \right) \cdot \B.
\end{dmath*}

The transpose of $D$ is then


\begin{dmath*}
  D^\top = \exp\left(\frac{-1}{2\cdot sd_g^2} * \left({\longones{K_g}
  \left(P^{2} *  
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix} \right)^\top
  } 
  + {(C_b^{2}\cdot 
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix}
  \cdot 
  \longones{|\Lambda|}
  ^\top )} 
  -2\cdot C_b * P ^\top \right)  \right) \cdot \B.
\end{dmath*}


Now, with the deformation, we can calculate the $K_p^\beta$ matrix in a similar way, with:

\begin{dmath*}
  K_p^\beta = \exp\left(\frac{-1}{2\cdot sd_p^2} * \left({(P-D)^{2}\cdot 
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix} \cdot 
  \longones{K_p}^\top} 
  + {(C_\A^{2}\cdot 
  \begin{bmatrix}
    1\\
    1
  \end{bmatrix}
  \cdot 
  \longones{|\Lambda|}
  ^\top )^\top} 
  -2\cdot (P-D)\cdot C_\A^\top \right)  \right).
\end{dmath*}

Now, consider the right side of the optimization problem

\begin{dmath*}
  R = \frac{1}{2sd_l^2} 
  \left\| 
  y - K_p^\B\A
  \right\|^2
\end{dmath*}

Taking the derivative, we get 

\begin{dmath*}
  \frac{\partial R}{\partial \B} = 
    \frac{1}{sd_p^2 * sd_l ^2} *
    K^\top * diag(y-K_p^\B\A) *
    K_p^\B * diag(\A) * C_\A 
    -\frac{1}{sd_p^2 * sd_l ^2} * K^\top * diag\left(
      (y-K_p^\B\A) \odot \left(
        K_p^\B * \left(
          \A \odot \longones{K_p}
        \right)
      \right)
    \right) * (P-K*\B) * diag(
      \begin{bmatrix}
      1\\
      1
      \end{bmatrix}).
\end{dmath*}


Simplifying yields
\begin{dmath*}
  \frac{\partial R}{\partial \B} = 
    \frac{1}{sd_p^2 * sd_l ^2} *
    K^\top \left(diag(y-K_p^\B\A) *
    K_p^\B * diag(\A) * C_\A 
    - diag\left(
      (y-K_p^\B\A) \odot \left(
        K_p^\B \A 
      \right)
    \right) * (P-K*\B)\right)
\end{dmath*}

Now, the tool gives a different representation of the gradient of we instead write:

\begin{dmath*}
  K_p^\beta = \exp\left(\frac{-1}{2\cdot sd_p^2} * \left({(P-D)^{2}\cdot 
  M1} 
  + {(C_\A^{2}\cdot 
  M2 )^\top} 
  -2\cdot (P-D)\cdot C_\A^\top \right)  \right).
\end{dmath*}
Notice we simply replaced a few matrices with $M1$ and $M2$.

With this, the tool gives the gradient as,
\begin{dmath*}
  \frac{\partial R}{\partial \B} = 
    \frac{1}{sd_p^2 * sd_l ^2} *
    K^\top * diag(y-K_p^\B\A) *
    K_p^\B * diag(\A) * C_\A 
    -\frac{1}{sd_p^2 * sd_l ^2} * K^\top * diag(y-K_p^\B\A) * ((
        K_p^\B * diag(\A) *M1^\top) \odot 
      (P-K*\B))
\end{dmath*}

Replacing $M1$ back, we get 

\begin{dmath*}
  \frac{\partial R}{\partial \B} = 
    \frac{1}{sd_p^2 * sd_l ^2} *
    K^\top * diag(y-K_p^\B\A) *
    K_p^\B * diag(\A) * C_\A 
    -\frac{1}{sd_p^2 * sd_l ^2} * K^\top * diag(y-K_p^\B\A) * ((
        K_p^\B * diag(\A) *(
        \begin{bmatrix}
          1\\
          1
        \end{bmatrix} \cdot 
        \longones{K_p}^\top) ^\top) \odot 
      (P-K*\B)),
\end{dmath*}

which simplifies to
\begin{dmath*}
  \frac{\partial R}{\partial \B} = 
    \frac{1}{sd_p^2 * sd_l ^2} *
    K^\top * diag(y-K_p^\B\A) *
    K_p^\B * diag(\A) * C_\A 
    -\frac{1}{sd_p^2 * sd_l ^2} * K^\top * diag(y-K_p^\B\A) * ((
        K_p^\B\A *
        \begin{bmatrix}
          1\\
          1
        \end{bmatrix}^\top)  \odot 
      (P-K*\B))
\end{dmath*}

The first term is the same, but the second term is slightly jumbled up.

The element-wise multiplication $\odot$ does not have nice associativity properties with the usual matrix multiplication $*$, so it is not apparent that this is same gradient as the one give earlier. 



\end{document}
